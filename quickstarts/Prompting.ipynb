{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Dummy data for demonstration\n",
        "data = {\n",
        "    'product': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'A'],\n",
        "    'sales': [100, 150, 120, 80, 200, 110, 90, 130],\n",
        "    'region': ['East', 'West', 'East', 'North', 'South', 'West', 'North', 'East']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define a simple tool for the AI agent to interact with data\n",
        "def query_data(query_string):\n",
        "    \"\"\"\n",
        "    Executes a simple query on the DataFrame and returns the result.\n",
        "    Example query_string: 'df[df[\"sales\"] > 100].to_json()'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = eval(query_string)\n",
        "        # Convert numpy types to standard Python types for JSON serialization\n",
        "        if isinstance(result, (pd.Series, pd.DataFrame)):\n",
        "            return result.to_json()\n",
        "        elif isinstance(result, list):\n",
        "            return json.dumps(result)\n",
        "        elif isinstance(result, (int, float, str, dict, bool)):\n",
        "            return json.dumps(result)\n",
        "        else:\n",
        "            return json.dumps(str(result)) # Fallback for other types\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": str(e)})\n",
        "\n",
        "# Simulate an LLM's ability to call tools\n",
        "def simulate_llm_tool_call(user_query):\n",
        "    if \"total sales\" in user_query.lower():\n",
        "        return \"query_data('df[\\\"sales\\\"].sum().item()')\"\n",
        "    elif \"sales by product\" in user_query.lower():\n",
        "        return \"query_data('df.groupby(\\\"product\\\")[\\\"sales\\\"].sum().to_json()')\"\n",
        "    elif \"products in east region\" in user_query.lower():\n",
        "        return \"query_data('df[df[\\\"region\\\"] == \\\"East\\\"][\\\"product\\\"].tolist()')\"\n",
        "    elif \"average sales\" in user_query.lower():\n",
        "        return \"query_data('df[\\\"sales\\\"].mean().item()')\"\n",
        "    elif \"raw data\" in user_query.lower():\n",
        "        return \"query_data('df.to_json(orient=\\\"records\\\")')\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def run_ai_agent(user_question):\n",
        "    print(f\"User: {user_question}\")\n",
        "    tool_call_string = simulate_llm_tool_call(user_question)\n",
        "\n",
        "    if tool_call_string:\n",
        "        print(f\"Agent (simulated LLM): Decided to call tool with: {tool_call_string}\")\n",
        "        # Execute the tool call\n",
        "        tool_name, args_str = tool_call_string.split('(', 1)\n",
        "        args_str = args_str[:-1] # Remove trailing ')'\n",
        "\n",
        "        # In a real scenario, you'd parse args_str more robustly\n",
        "        # For this simple example, we know it's a single string argument\n",
        "        arg = args_str.strip(\"'\")\n",
        "\n",
        "        if tool_name == \"query_data\":\n",
        "            tool_result = query_data(arg)\n",
        "            print(f\"Tool (query_data) Result: {tool_result}\")\n",
        "            # Simulate LLM processing the tool result and generating a natural language response\n",
        "            if \"total sales\" in user_question.lower():\n",
        "                total_sales = json.loads(tool_result)\n",
        "                print(f\"Agent: The total sales are {total_sales}.\")\n",
        "            elif \"sales by product\" in user_question.lower():\n",
        "                sales_by_product = json.loads(tool_result)\n",
        "                print(f\"Agent: Sales by product are: {sales_by_product}\")\n",
        "            elif \"products in east region\" in user_question.lower():\n",
        "                products = json.loads(tool_result)\n",
        "                print(f\"Agent: Products in the East region are: {', '.join(products)}.\")\n",
        "            elif \"average sales\" in user_question.lower():\n",
        "                avg_sales = json.loads(tool_result)\n",
        "                print(f\"Agent: The average sales are {avg_sales:.2f}.\")\n",
        "            elif \"raw data\" in user_question.lower():\n",
        "                raw_data = json.loads(tool_result)\n",
        "                print(f\"Agent: Here is the raw data: {raw_data}\")\n",
        "            else:\n",
        "                print(f\"Agent: I processed the data. Here is the raw result: {tool_result}\")\n",
        "        else:\n",
        "            print(\"Agent: Unknown tool called.\")\n",
        "    else:\n",
        "        print(\"Agent: I cannot answer that question with my current tools.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n--- Data Analyst AI Agent Simulation ---\")\n",
        "    run_ai_agent(\"What are the total sales?\")\n",
        "    print(\"\\n\")\n",
        "    run_ai_agent(\"Show me sales by product.\")\n",
        "    print(\"\\n\")\n",
        "    run_ai_agent(\"List products in the East region.\")\n",
        "    print(\"\\n\")\n",
        "    run_ai_agent(\"What is the average sales?\")\n",
        "    print(\"\\n\")\n",
        "    run_ai_agent(\"Show me the raw data.\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "vmQ-9wd7ZCbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: Prompting Quickstart\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Prompting.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpOYALec6N8Z"
      },
      "source": [
        "This notebook contains examples of how to write and run your first prompts with the Gemini API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0c13de5f68f6",
        "outputId": "d5dcf93a-be26-4c51-fd76-995f8f6bfa5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.3/257.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U -q \"google-genai>=1.4.0\" # 1.4.0 is needed for chat history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4YDYyfRYN7L"
      },
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p8K1RpmMfh20",
        "outputId": "bb997533-d596-4f06-d4da-95fc0d628447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret GOOGLE_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2539307896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mGOOGLE_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GOOGLE_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGOOGLE_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret GOOGLE_API_KEY does not exist."
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from google import genai\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwtL8wmX4rqP"
      },
      "source": [
        "## Select your model\n",
        "\n",
        "Now select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. [thinking notebook](./Get_started_thinking.ipynb) for more details and in particular learn how to switch the thiking off)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll79uwEK4uPJ"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\" # @param [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash-lite-preview-09-2025\", \"gemini-2.5-flash\", \"gemini-2.5-flash-preview-09-2025\", \"gemini-2.5-pro\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTNQymX8YN9c"
      },
      "source": [
        "## Run your first prompt\n",
        "\n",
        "Use the `generate_content` method to generate responses to your prompts. You can pass text directly to generate_content, and use the `.text` property to get the text content of the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSuyaGmcf6sr"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Give me python code to sort a list\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GTyrWHugKFi"
      },
      "source": [
        "## Use images in your prompt\n",
        "\n",
        "Here you will download an image from a URL and pass that image in our prompt.\n",
        "\n",
        "First, you download the image and load it with PIL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgbFtil0gLNf"
      },
      "outputs": [],
      "source": [
        "!curl -o image.jpg \"https://storage.googleapis.com/generativeai-downloads/images/jetpack.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rcYDbcDga8s"
      },
      "outputs": [],
      "source": [
        "import PIL.Image\n",
        "img = PIL.Image.open('image.jpg')\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTgRAmEHOaAz"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "    This image contains a sketch of a potential product along with some notes.\n",
        "    Given the product sketch, describe the product as thoroughly as possible based on what you\n",
        "   see in the image, making sure to note all of the product features. Return output in json format:\n",
        "   {description: description, features: [feature1, feature2, feature3, etc]}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJyRsfQi0tp6"
      },
      "source": [
        "Then you can include the image in our prompt by just passing a list of items to `generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aoil5YiTgbZS"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[prompt, img],\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE-6e7gePN7Q"
      },
      "source": [
        "## Have a chat\n",
        "\n",
        "The Gemini API enables you to have freeform conversations across multiple turns.\n",
        "\n",
        "The [ChatSession](https://ai.google.dev/api/python/google/generativeai/ChatSession) class will store the conversation history for multi-turn interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKAtY5oIPQW0"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tXNVnqxPcXy"
      },
      "outputs": [],
      "source": [
        "response = chat.send_message(\n",
        "    message=\"In one sentence, explain how a computer works to a young child.\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TChH2l5PhFf"
      },
      "source": [
        "You can see the chat history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHwrC82YPiWS"
      },
      "outputs": [],
      "source": [
        "messages = chat.get_history()\n",
        "for message in messages:\n",
        "  print(f\"{message.role}: {message.parts[0].text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvHvt1OEPl7D"
      },
      "source": [
        "You can keep sending messages to continue the conversation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fXZZQPzPkie"
      },
      "outputs": [],
      "source": [
        "response = chat.send_message(\"Okay, how about a more detailed explanation to a high schooler?\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65476e75ece0"
      },
      "source": [
        "## Set the temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56f68c900144"
      },
      "source": [
        "Every prompt you send to the model includes parameters that control how the model generates responses. Use a `types.GenerateContentConfig` to set these, or omit it to use the defaults.\n",
        "\n",
        "Temperature controls the degree of randomness in token selection. Use higher values for more creative responses, and lower values for more deterministic responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3c68071ed8b"
      },
      "source": [
        "Note: Although you can set the `candidate_count` in the generation_config, 2.0 and later models will only return a single candidate at the this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c97c16e6a961"
      },
      "outputs": [],
      "source": [
        "from google.genai import types\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents='Give me a numbered list of cat facts.',\n",
        "    config=types.GenerateContentConfig(\n",
        "        max_output_tokens=2000,\n",
        "        temperature=1.9,\n",
        "        stop_sequences=['\\n6'] # Limit to 5 facts.\n",
        "    )\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvkDhXtHgol7"
      },
      "source": [
        "## Learn more\n",
        "\n",
        "There's lots more to learn!\n",
        "\n",
        "* For more fun prompts, check out [Market a Jetpack](https://github.com/google-gemini/cookbook/blob/main/examples/Market_a_Jet_Backpack.ipynb).\n",
        "* Check out the [safety quickstart](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Safety.ipynb) next to learn about the Gemini API's configurable safety settings, and what to do if your prompt is blocked.\n",
        "* For lots more details on using the Python SDK, check out the [get started notebook](./Get_started.ipynb) or the [documentation's quickstart](https://ai.google.dev/tutorials/python_quickstart)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Prompting.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}